
@article{baumgartnerCRAFTSharedTasks2019,
  title = {{{CRAFT Shared Tasks}} 2019 {{Overview}} \textendash - {{Integrated Structure}}, {{Semantics}}, and {{Coreference}}},
  author = {Baumgartner, William and Bada, Michael and Pyysalo, Sampo and Ciosici, Manuel R. and Hailu, Negacy and {Pielke-Lombardo}, Harrison and Regan, Michael and Hunter, Lawrence},
  year = {2019},
  journal = {Proceedings of The 5th Workshop on BioNLP Open Shared Tasks},
  pages = {174--184},
  doi = {10.18653/v1/d19-5725},
  abstract = {As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks-dependency parse construction, coref-erence resolution, and ontology concept identification over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task fo-cuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks.},
  copyright = {All rights reserved},
  keywords = {Challenge,Concept recognition,Coreference resolution,CRAFT,Entity recognition,HPL,Hunter Lab,Natural Language Processing,Relation Extraction,Syntax parsing},
  annotation = {ZSCC: 0000001},
  file = {/home/harrison/Dropbox/PDFs/baumgartnerCRAFTSharedTasks2019.pdf}
}

@article{callahanKnowledgeBasedBiomedicalData2020,
  ids = {callahanKnowledgebasedBiomedicalData2019},
  title = {Knowledge-{{Based Biomedical Data Science}}},
  author = {Callahan, Tiffany J. and Tripodi, Ignacio J. and {Pielke-Lombardo}, Harrison and Hunter, Lawrence E.},
  year = {2020},
  journal = {Annual Review of Biomedical Data Science},
  volume = {3},
  number = {1},
  pages = {23--41},
  doi = {10.1146/annurev-biodatasci-010820-091627},
  abstract = {Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.},
  copyright = {All rights reserved},
  keywords = {explanation,Graph Embeddings,HPL,inference,Knowledge Discovery,Knowledge Embeddings,Knowledge Graphs,Knowledge Representation,Machine Learning,Natural Language Processing,Ontology,Reasoning,Semantic Web,text mining},
  annotation = {ZSCC: 0000003  \_eprint: https://doi.org/10.1146/annurev-biodatasci-010820-091627},
  file = {/home/harrison/Dropbox/PDFs/callahanKnowledgeBasedBiomedicalData2020.pdf}
}

@article{powersGSEAInContextIdentifyingNovel2018,
  ids = {powersGSEAInContextIdentifyingNovel},
  title = {{{GSEA-InContext}}: {{Identifying}} Novel and Common Patterns in Expression Experiments},
  shorttitle = {{{GSEA-InContext}}},
  author = {Powers, Rani K. and Goodspeed, Andrew and {Pielke-Lombardo}, Harrison and Tan, Aik Choon and Costello, James C.},
  year = {2018},
  journal = {Bioinformatics},
  volume = {34},
  number = {13},
  pages = {i555--i564},
  issn = {14602059},
  doi = {10.1093/bioinformatics/bty271},
  abstract = {Motivation: Gene Set Enrichment Analysis (GSEA) is routinely used to analyze and interpret coordinate pathway-level changes in transcriptomics experiments. For an experiment where less than seven samples per condition are compared, GSEA employs a competitive null hypothesis to test significance. A gene set enrichment score is tested against a null distribution of enrichment scores generated from permuted gene sets, where genes are randomly selected from the input experiment. Looking across a variety of biological conditions, however, genes are not randomly distributed with many showing consistent patterns of up- or down-regulation. As a result, common patterns of positively and negatively enriched gene sets are observed across experiments. Placing a single experiment into the context of a relevant set of background experiments allows us to identify both the common and experiment-specific patterns of gene set enrichment. Results: We compiled a compendium of 442 small molecule transcriptomic experiments and used GSEA to characterize common patterns of positively and negatively enriched gene sets. To identify experiment-specific gene set enrichment, we developed the GSEA-InContext method that accounts for gene expression patterns within a background set of experiments to identify statistically significantly enriched gene sets. We evaluated GSEA-InContext on experiments using small molecules with known targets to show that it successfully prioritizes gene sets that are specific to each experiment, thus providing valuable insights that complement standard GSEA analysis.},
  copyright = {All rights reserved},
  keywords = {HPL},
  annotation = {ZSCC: 0000006},
  file = {/home/harrison/Dropbox/PDFs/powersGSEAInContextIdentifyingNovel2018.pdf}
}

@incollection{reinholdPreanalyticConsiderationsMass2019,
  title = {Pre-Analytic Considerations for Mass Spectrometry-Based Untargeted Metabolomics Data},
  author = {Reinhold, Dominik and {Pielke-Lombardo}, Harrison and Jacobson, Sean and Ghosh, Debashis and Kechris, Katerina},
  year = {2019},
  volume = {1978},
  abstract = {Metabolomics is the science of characterizing and quantifying small molecule metabolites in biological systems. These metabolites give organisms their biochemical characteristics, providing a link between genotype, environment, and phenotype. With these opportunities also come data challenges, such as compound annotation, missing values, and batch effects. We present the steps of a general pipeline to process untargeted mass spectrometry data to alleviate the latter two challenges. We assume to have a matrix with metabolite abundances, with metabolites in rows and samples in columns. The steps in the pipeline include summarizing technical replicates (if available), filtering, imputing, transforming, and normalizing the data. In each of these steps, a method and parameters should be chosen based on assumptions one is willing to make, the question of interest, and diagnostic tools. Besides giving a general pipeline that can be adapted by the reader, our goal is to review diagnostic tools and criteria that are helpful when making decisions in each step of the pipeline and assessing the effectiveness of normalization and batch correction. We conclude by giving a list of useful packages and discuss some alternative approaches that might be more appropriate for the reader's data.},
  copyright = {All rights reserved},
  keywords = {\#nosource,Filtering,HPL,Imputation,Mass spectrometry,Metabolomics,Normalization,Pre-analytic,Processing,Technical replicates,Untargeted},
  annotation = {ZSCC: 0000001},
  file = {/home/harrison/Dropbox/PDFs/reinholdPreanalyticConsiderationsMass2019.pdf}
}


